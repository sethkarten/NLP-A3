main.py --train --cond --batch_method translation --attn

Building data from ./data...
      batch_size: 20
batch_size_valid: 60
    batch_method: translation      (no sorting by target lengths)
          device: cpu
  is_conditional: True

15 batches
5 batches

vocab_size: 1282

train.txt
              # words: 6681
               # seqs: 300
  avg/max/min lengths: 22/72/3

src-train.txt
              # words: 6081
               # seqs: 300
  avg/max/min lengths: 20/70/1

Seq2Seq
      # parameters: 522682
        vocab_size: 1282
               dim: 100
          # layers: 2
    is_conditional: 1
     bidirectional: 0
        use_bridge: 0
     use_attention: 1

Control
            lr: 20.00
          bptt: 35

| epoch   1 |    20/   29 batches | lr 20.00 | ms/batch 108.73 | loss  7.52 | ppl  1839.65
-----------------------------------------------------------------------------------------
| end of epoch   1 | time:  3.88s | valid loss  5.93 | valid ppl   375.58 | valid sqxent   126.10
-----------------------------------------------------------------------------------------
| epoch   2 |    20/   29 batches | lr 20.00 | ms/batch 104.08 | loss  5.90 | ppl   365.71
-----------------------------------------------------------------------------------------
| end of epoch   2 | time:  3.79s | valid loss  5.59 | valid ppl   267.66 | valid sqxent   118.89
-----------------------------------------------------------------------------------------
| epoch   3 |    20/   29 batches | lr 20.00 | ms/batch 107.19 | loss  5.20 | ppl   181.90
-----------------------------------------------------------------------------------------
Exiting from training early
=========================================================================================
| End of training | final loss  5.59 | final ppl   267.66 | final sqxent   118.89
=========================================================================================
00:00:11
